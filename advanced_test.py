#!/usr/bin/env python3
"""
Advanced test for ModelTrainer with realistic diamond data and hyperparameter tuning.
"""

import os
import sys
import numpy as np
import pandas as pd
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import xgboost as xgb

# Add src to path
sys.path.append('src')


def create_realistic_diamond_data(n_samples=5000):
    """Create realistic synthetic diamond dataset with proper correlations."""
    print(f"Creating realistic diamond dataset with {n_samples} samples...")
    
    np.random.seed(42)
    
    # Generate carat with realistic distribution
    carat = np.random.lognormal(mean=0.2, sigma=0.6, size=n_samples)
    carat = np.clip(carat, 0.2, 5.0)
    
    # Cut quality (encoded as ordinal: Fair=1, Good=2, Very Good=3, Premium=4, Ideal=5)
    cut_probs = [0.05, 0.15, 0.25, 0.35, 0.2]  # Distribution of cut quality
    cut = np.random.choice([1, 2, 3, 4, 5], size=n_samples, p=cut_probs)
    
    # Color (D=7, E=6, F=5, G=4, H=3, I=2, J=1)
    color_probs = [0.05, 0.1, 0.15, 0.25, 0.2, 0.15, 0.1]
    color = np.random.choice([1, 2, 3, 4, 5, 6, 7], size=n_samples, p=color_probs)
    
    # Clarity (I1=1, SI2=2, SI1=3, VS2=4, VS1=5, VVS2=6, VVS1=7, IF=8)
    clarity_probs = [0.05, 0.15, 0.20, 0.25, 0.15, 0.10, 0.08, 0.02]
    clarity = np.random.choice([1, 2, 3, 4, 5, 6, 7, 8], size=n_samples, p=clarity_probs)
    
    # Depth and Table (correlated with quality)
    depth = np.random.normal(61.5, 1.5, n_samples)
    depth = np.clip(depth, 55, 70)
    
    table = np.random.normal(57, 2, n_samples)  
    table = np.clip(table, 50, 65)
    
    # Physical dimensions (strongly correlated with carat)
    x = (carat ** (1/3)) * 6.2 + np.random.normal(0, 0.2, n_samples)
    y = x + np.random.normal(0, 0.1, n_samples)  # y slightly different from x
    z = x * 0.618 + np.random.normal(0, 0.1, n_samples)  # Golden ratio for aesthetics
    
    # Ensure positive dimensions
    x = np.maximum(x, 0.1)
    y = np.maximum(y, 0.1) 
    z = np.maximum(z, 0.1)
    
    # Create realistic price model
    # Base price from carat (polynomial relationship)
    base_price = (carat ** 1.8) * 3500
    
    # Quality multipliers
    cut_multiplier = [0.85, 0.92, 1.0, 1.08, 1.15]
    color_multiplier = [0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3]
    clarity_multiplier = [0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3]
    
    price = []
    for i in range(n_samples):
        quality_adj = (
            cut_multiplier[cut[i]-1] * 
            color_multiplier[color[i]-1] * 
            clarity_multiplier[clarity[i]-1]
        )
        
        # Dimension quality factor
        dim_factor = 1 + (x[i] * y[i] * z[i] - np.mean(x * y * z)) / np.std(x * y * z) * 0.1
        
        final_price = base_price[i] * quality_adj * dim_factor * (1 + np.random.normal(0, 0.15))
        price.append(max(300, final_price))
    
    # Create DataFrame
    df = pd.DataFrame({
        'carat': carat,
        'cut': cut,
        'color': color,
        'clarity': clarity,
        'depth': depth,
        'table': table,
        'x': x,
        'y': y,
        'z': z,
        'price': price
    })
    
    print(f"Dataset created with shape: {df.shape}")
    print(f"Price range: ${df['price'].min():.0f} - ${df['price'].max():.0f}")
    print(f"Mean price: ${df['price'].mean():.0f}")
    
    return df


def advanced_model_training(X_train, X_test, y_train, y_test):
    """Train XGBoost with comprehensive hyperparameter optimization."""
    
    print("\nPerforming hyperparameter optimization...")
    
    # Define parameter grid for optimization
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [4, 6, 8],
        'learning_rate': [0.05, 0.1, 0.15],
        'subsample': [0.8, 0.9],
        'colsample_bytree': [0.8, 0.9],
        'reg_alpha': [0, 0.1],
        'reg_lambda': [1, 1.5]
    }
    
    # Base model
    base_model = xgb.XGBRegressor(random_state=42)
    
    # Grid search with cross-validation
    grid_search = GridSearchCV(
        estimator=base_model,
        param_grid=param_grid,
        scoring='r2',
        cv=5,
        n_jobs=-1,
        verbose=1
    )
    
    print("Running grid search (this may take a few minutes)...")
    grid_search.fit(X_train, y_train)
    
    # Best model
    best_model = grid_search.best_estimator_
    
    print(f"\nBest parameters: {grid_search.best_params_}")
    print(f"Best CV score: {grid_search.best_score_:.4f}")
    
    # Final predictions
    y_pred = best_model.predict(X_test)
    
    # Calculate metrics
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    
    return {
        'model': best_model,
        'best_params': grid_search.best_params_,
        'best_cv_score': grid_search.best_score_,
        'r2_score': r2,
        'mae': mae,
        'rmse': rmse,
        'mape': mape
    }


def test_advanced_modeltrainer():
    """Advanced test with realistic data and hyperparameter optimization."""
    
    print("="*70)
    print("ADVANCED MODEL TRAINER TEST - TARGET: 95%+ RÂ² ACCURACY")
    print("="*70)
    
    # 1. Create realistic diamond data
    print("\nStep 1: Creating realistic diamond dataset...")
    df = create_realistic_diamond_data(5000)
    
    # Show some statistics
    print("\nDataset Statistics:")
    print(df.describe())
    
    # 2. Feature engineering
    print("\nStep 2: Feature engineering...")
    
    # Create additional features
    df['volume'] = df['x'] * df['y'] * df['z']
    df['carat_volume_ratio'] = df['carat'] / df['volume']
    df['price_per_carat'] = df['price'] / df['carat']
    
    # Quality score (composite feature)
    df['quality_score'] = (df['cut'] * 0.3 + df['color'] * 0.3 + df['clarity'] * 0.4) / 3
    
    print(f"Enhanced features: {list(df.columns)}")
    
    # 3. Prepare data
    print("\nStep 3: Preparing training data...")
    
    feature_cols = ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 
                   'x', 'y', 'z', 'volume', 'carat_volume_ratio', 'quality_score']
    
    X = df[feature_cols].values
    y = df['price'].values
    
    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Split data (80/20 split for more training data)
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=None
    )
    
    print(f"Training set: {X_train.shape}")
    print(f"Test set: {X_test.shape}")
    
    # 4. Advanced model training
    print("\nStep 4: Advanced XGBoost training with hyperparameter optimization...")
    
    results = advanced_model_training(X_train, X_test, y_train, y_test)
    
    # 5. Results analysis
    print("\n" + "="*70)
    print("FINAL RESULTS")
    print("="*70)
    
    print(f"\nModel Performance:")
    print(f"  RÂ² Score: {results['r2_score']:.6f}")
    print(f"  MAE: ${results['mae']:.2f}")
    print(f"  RMSE: ${results['rmse']:.2f}")
    print(f"  MAPE: {results['mape']:.2f}%")
    print(f"  CV Score: {results['best_cv_score']:.6f}")
    
    # Target achievement check
    target_achieved = results['r2_score'] >= 0.95
    print(f"\nTarget Achievement (RÂ² >= 0.95): {'SUCCESS' if target_achieved else 'NEEDS IMPROVEMENT'}")
    
    if target_achieved:
        print("ðŸŽ‰ EXCELLENT! Model achieved 95%+ accuracy target!")
        print("Model is ready for production deployment.")
    else:
        print(f"ðŸ“Š Current accuracy: {results['r2_score']:.4f} (Target: 0.95)")
        print("Consider: more data, advanced feature engineering, or ensemble methods.")
    
    # Best hyperparameters
    print(f"\nBest Hyperparameters:")
    for param, value in results['best_params'].items():
        print(f"  {param}: {value}")
    
    # Feature importance
    if hasattr(results['model'], 'feature_importances_'):
        importance = results['model'].feature_importances_
        feature_importance = list(zip(feature_cols, importance))
        feature_importance.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\nFeature Importance (Top 5):")
        for feature, imp in feature_importance[:5]:
            print(f"  {feature}: {imp:.4f}")
    
    # Save model
    print("\nStep 5: Saving model...")
    os.makedirs("artifacts", exist_ok=True)
    
    import pickle
    model_path = "artifacts/advanced_diamond_model.pkl"
    with open(model_path, "wb") as f:
        pickle.dump({
            'model': results['model'],
            'scaler': scaler,
            'feature_columns': feature_cols,
            'performance': {
                'r2_score': results['r2_score'],
                'mae': results['mae'],
                'rmse': results['rmse'],
                'target_achieved': target_achieved
            }
        }, f)
    
    print(f"Model saved to {model_path}")
    
    return results


if __name__ == "__main__":
    # Create directories
    os.makedirs("artifacts", exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    
    # Run advanced test
    try:
        results = test_advanced_modeltrainer()
        
        print(f"\n{'='*70}")
        print("FINAL SUMMARY")
        print(f"{'='*70}")
        print(f"RÂ² Score: {results['r2_score']:.6f}")
        print(f"Target Met (>=0.95): {'âœ“ YES' if results['r2_score'] >= 0.95 else 'âœ— NO'}")
        print(f"MAE: ${results['mae']:.2f}")
        print(f"RMSE: ${results['rmse']:.2f}")
        print("\nAdvanced ModelTrainer test completed!")
        
    except Exception as e:
        print(f"Test failed with error: {e}")
        import traceback
        traceback.print_exc()